{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use DataFrames versus RDDs**\n",
    "\n",
    "\n",
    "RDD requires user to specify schema of data while dataframe is organized into named columnds. Through dataframe, we can process structured and unstructured data efficiently. It also allows Spark to manage schema.\n",
    "\n",
    "RDDs are immutable in nature. That means we can not change anything about RDDs. \n",
    "\n",
    "RDD can be taken from any source while DataFrame are from set formats, such as csv, json, hdfs.\n",
    "\n",
    "Dataframes are more effecient and faster to use.\n",
    "\n",
    "Use RDD When you want low-level transformation and actions and When you want low-level transformation and actions you use DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two ways to create an RDD using a SparkContext: Parallelize and read from an external data source\n",
    "\n",
    "#Parallelize\n",
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "\n",
    "#External\n",
    "distFile = sc.textFile(\"data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some Common transformations and actions\n",
    "\n",
    "lines = sc.textFile(\"data.txt\")\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create a data frame from a JSON file\n",
    "empDf = sqlContext.read.json(\"customerData.json\")\n",
    "\n",
    "#create a data frame from a list\n",
    " deptList = [{'name': 'Sales', 'id': \"100\"},\\\n",
    "     { 'name':'Engineering','id':\"200\" }]\n",
    " deptDf = sqlContext.createDataFrame(deptList)\n",
    "\n",
    "#Cache a DataFrame into memory\n",
    " empDf.cache()\n",
    "\n",
    "#Run some operations on DataFrames\n",
    "#cascading operations\n",
    "empDf.filter(empDf[\"age\"] >30).join(deptDf, \\\n",
    "        empDf.deptid == deptDf.id).\\\n",
    "        groupBy(\"deptid\").\\\n",
    "        agg({\"salary\": \"avg\", \"age\": \"max\"}).show()\n",
    "\n",
    "#Join two DataFrames\n",
    " empDf.join(deptDf, empDf.deptid == deptDf.id).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common use cases for shared variables**\n",
    "\n",
    "Two types Broadcast variables and Accumulators\n",
    "\n",
    "Broadcast variables are used to give a large dataset as input to every node in an effecient manner.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Broadcast variables\n",
    "\n",
    "broadcastVar = sc.broadcast([1, 2, 3])\n",
    "\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulators are effeciently used as counters, used in mapreduce and sums and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accumulators \n",
    "# How accumulators can be used to implement distributed counters in parallel\n",
    "accum = sc.accumulator(0)\n",
    "accum\n",
    "\n",
    "sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))\n",
    "\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using broadcast variables to keep a read-only variable cached on each machine\n",
    "\n",
    "import scala.math._\n",
    "val myRdd = sc.parallelize(Array(1,2,3,4,5,6,7))\n",
    "val power =  spark.sparkContext.broadcast(3)\n",
    "myRdd.foreach(pow(_,power))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6005fd90017a15c40125a1ff35e4ba5456a924b790bb85f87edd5ab98de4ea18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
